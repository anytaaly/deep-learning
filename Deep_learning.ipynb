{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqvTx00eQ5ch6Qrdh2HpNF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anytaaly/deep-learning/blob/main/Deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFRvMKwmjWtU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "\n",
        "\n",
        "*   PATTERN RECOGNITION\n",
        "AND MACHINE LEARNING\n",
        "CHRISTOPHER M. BISHOОР\n",
        "https://www.microsoft.com/en-us/research/wp-content/uploads/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf\n",
        "\n",
        "*   A LOGICAL CALCULUS OF THE IDEAS IMMANENT IN\n",
        "NERVOUS ACTIVITY* https://www.cs.cmu.edu/~epxing/Class/10715/reading/McCulloch.and.Pitts.pdf\n",
        "\n",
        "\n",
        "* Goodfellow chapter 1\n",
        "https://www.deeplearningbook.org/\n",
        "\n",
        "* Computing machinery and intelligence by Alan Turning in 1960 (Turing, 2009): https://courses.cs.umbc.edu/471/papers/turing.pdf\n",
        "\n",
        "* A LOGICAL CALCULUS OF THE IDEAS IMMANENT IN\n",
        "NERVOUS ACTIVITY  https://www.cs.cmu.edu/~epxing/Class/10715/reading/McCulloch.and.Pitts.pdf\n",
        "\n",
        "\n",
        "2nd Phase of Perceptron -\n",
        "* Learning Internal Representations by Error Propagation Rumelhart et al.\n",
        "https://apps.dtic.mil/sti/html/tr/ADA164453/\n",
        "https://apps.dtic.mil/sti/tr/pdf/ADA164453.pdf\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "sKzE1WihrhpF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Warren McCulloch (Neuroscientist) and Walter Pitts (mathemethician) built the first model of a mechanical neuron and propounded the idea that simple elemental computational blocks in your brain work together to perform complex functions.\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1ymT00yL0lWX3a3fIhKUmP5fF_D3PDhN8)\n",
        "  \n",
        "  Here is the link of their paper: A LOGICAL CALCULUS OF THE IDEAS IMMANENT IN NERVOUS ACTIVITY https://www.cs.cmu.edu/~epxing/Class/10715/reading/McCulloch.and.Pitts.pdf\n",
        "\n",
        "\n",
        "  What McCulloch and Walter interested in was taking these biological neurons and then abstracting it out into some mathematical object that they could understand a little better.\n",
        "\n",
        "  we do not want to necessarily always model such complex flow in order to build a model.\n",
        "\n",
        "  So, what McCulloch and Putts did is thta they said that , let neuron be a simple function. It takes n inputs and produces one output.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "7m-N7Ipf3gQz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7n-qAQINm21j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1-Representaton Learning\n",
        "Perceptual agents, from plants to humans, perform measurements of physical processes (\"signals\") ayt a level of granularity that is essentially continuous."
      ],
      "metadata": {
        "id": "TQTTM0ezm3R8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vvj3NtLhnRT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2- Linear Classifier\n",
        "\n",
        "\n",
        "$$\n",
        "f(x;w) = sign(w^T x) = sign(w_1x_1 + .... + w_dx_d)\n",
        "$$\n",
        "\n",
        "This is the equation of a linear classifier and also the mathematical form of a single artificial neuron / perceptron.\n",
        "\n",
        "\n",
        "Where\n",
        "\n",
        "1-  **x** is the data point. Example: a person described by features like height, weight, age, etc.\n",
        "so each $x_i$ is one feature.\n",
        "$$\n",
        "x = (x_1, x_2,x_3,....,x_d)  \n",
        "$$\n",
        "\n",
        "\n",
        "2- w - the weights (parameters to learn)\n",
        "$$\n",
        "w= (w_1, w_2,w_3,....,w_d)  \n",
        "$$\n",
        "These are learned by the model. They represent how important each feature is.\n",
        "\n",
        "3- Big $|w_i|$ feature matters more.\n",
        "\n",
        "\n",
        "$ w^T x $ - weighted sum( dot product)\n",
        "$$ sign(w^T x) = sign(w_1x_1 + .... + w_dx_d) $$\n",
        "\n",
        "This is exactly what neuron's activation before nonlinearity.\n",
        "\n",
        "4- sing - the decision function\n",
        "\n",
        "$$ sign(z) = \\int_{-1\\, if \\,z>0}^{+1 \\,if \\,z>0} $$\n",
        "\n",
        "So after computing the weighted sum, we just check:\n",
        "\n",
        "\n",
        "*   Is it positive? -> class +1\n",
        "*   Is it negative? -> class -1\n",
        "\n",
        "This is exactly how:\n",
        "\n",
        "* Perceptron\n",
        "* Linear classifiers\n",
        "* SVM (core decision rule)\n",
        "* Logistic regression (before sigmoid)\n",
        "\n",
        "start.\n",
        "\n"
      ],
      "metadata": {
        "id": "UuifyqUypRNx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p7D3kuaj5pqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3- Linear Classifier On an Image:\n",
        "The above Linear classifier is also used on image classificagtion as well. When you use this model on an image, your input vector is:\n",
        "$$\n",
        "x = (x_1, x_2,x_3,....,x_d)  \n",
        "$$\n",
        "\n",
        "Each feature $ x_i$ is typically:\n",
        "The pixel value at a specific position in the image.\n",
        "So yes:\n",
        "\n",
        "\n",
        "*   a pixel = a feature\n",
        "*   its position is implicit in the index i\n",
        "*   the model does not see \"row 3, column 5\" - it just sees features #837\"\n",
        "\n",
        "Example: A 64 x 64 grascale image will have 4996 featyres. each $ x_i $ is tge intensity of one pixel. If it's RGB, then each controbutes 3 features (R,G,B)\n",
        "\n",
        "What this model is actually doing to an image is that it computes:\n",
        "$$ w^T x = \\sum_{i} w_i x_i $$\n",
        "Each pixel is multiplied by a learned weight. So the model is effectively learning a template image w and asking \"How similar is this image to my apple-vs.orange template?\"\n",
        "\n",
        "- Pixels that. matter more (edges, color zones, textures) get bigger weights.\n",
        "- Background pixels ideally get weights near zero.\n",
        "\n",
        "Although each raw feature is just a color intensity, the pattern across many pixels encodes:\n",
        "\n",
        "* edges\n",
        "* roundness\n",
        "* texture\n",
        "* shadows\n",
        "* highlights\n",
        "\n",
        "These are emergent features, not explicit ones.\n",
        "\n"
      ],
      "metadata": {
        "id": "dR5-UHnVs1aO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l6lKX0dL5qf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4- Back-propagation\n"
      ],
      "metadata": {
        "id": "9EjH-NDO52KC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vCT9bLKo5ruU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5- CNN - Convolutional Neural Network"
      ],
      "metadata": {
        "id": "3A5lVBZg6Wh4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FK8bdloc5sub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6- Problem Setup for Machine Learning:\n",
        "Nature gives us data X and targets Y for this data\n",
        "$$ X -> Y $$\n",
        "Nature does not usually tell us what property of a datu,m $ X \\in X $ results in a particular orediction $ y \\in Y $. Machine learning is domain where we try to imitate Natures pattern to predict y for a given x by learning through the available data.\n",
        "\n",
        "Machine learning simply implies a notion of being able to identify patterns in the input data without explicitly programming a computer for prediction.\n",
        "\n",
        "we are often happy with a learning process that identifies correlations: If we learn correlations on a few samples\n",
        "$$ (x^1, y^1), ..., (x^n, y^n) $$, we may be able to predict output for a new data $ (x^{n+1}) $\n",
        "\n",
        "We may not need to know why the label of $x^{n+1}$ was predicted to be so and so.\n",
        "\n",
        "Nature possesses a probability distribution P over (X,Y).\n",
        "\n",
        "We will formalize the problem of machine learning as Nature drawing n independent and identically distributed samples from this distribution. This is denoted by:\n",
        "$$\n",
        "D_{train} = \\{(x^i , y^i) \\sim P\\}^{n}_{i=1}\n",
        "$$\n",
        "is called the training set. We use this data to identify patterns that help make predictions on some future data.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "RdipnCjDxoq2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Designing a predictor that is accurate on $ D_{train} $ is trivial. A hash function that memorizes the data is sufficient. This is NOT our task in machine learning. We want predictors that generalize to new data outside $ D_{train} $.\n",
        "You should never get happy that you get zero error on training data set.\n",
        "\n",
        "\n",
        "The problem in machine learning: Give answer to questions you havent seen before:\n"
      ],
      "metadata": {
        "id": "dIEcNHFvF5me"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7- Generalization:\n",
        "\n",
        "Generalization: A model can predict well on data from the same distribution\n",
        "\n",
        "**Professor**: I shouldn't expect you to be good at answering questions from outside the training data unless there is some shared property in the training and the test data.\n",
        "\n",
        "In machine learning, we talk about one particular kind of shared property, the fact that training and testing may just come from the same probability distribution.\n",
        "\n",
        "The right class of functions f is such that is not too large. Otherwise we will find our binary classifier discussed above as the solution of the problem (after all, it does achieve zero training error). This is not very useful. The class of functions that we should search over cannot be too small either, otherwise we won’t be able to make accurate predictions for difficult images."
      ],
      "metadata": {
        "id": "Y-ksGtetICQi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AUt433gx5xfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8- Linear Regression: Reference Lectures\n",
        "\n",
        "* Quick Intro of linear Regression: https://www.youtube.com/watch?v=3dhcmeOTZ_Q\n",
        "* GPA Example - Linear Regression: https://www.youtube.com/watch?v=Muw2GjZ0Xww\n",
        "\n",
        "#9 - Linear Regresssion - UPenn ES5460\n",
        "\n",
        "We usually have images or input data in d dimension vector and our labels are simply real values in regression problem.\n",
        "\n",
        "Regression problems are simply problems that have some inputs, and we expect to regress the output, to find a real value number that is close to the true real value number that was given to you as a part of the training data set.\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1Bs7Bmuz7V7JSCC9fOqxr-VO6qMuW0teE)\n",
        "\n",
        "\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1xf6TDCNHTdEdANiFWthrvNw_NXAmHBSb)\n",
        "\n",
        "\n",
        "So, Machine Learning function is:\n",
        "\n",
        "$$\n",
        "f(x; w,b) = w^T + b\n",
        "$$\n",
        "\n",
        "B is an scalar value ( in lay man : That’s it. One value. Not a list. Not a vector. Not a matrix.)\n",
        "the function take x input and is parameterize by w and b and outputs y.\n",
        "\n",
        "## Example of Regression model:\n",
        "In Below diagram we have two dimensions - Lets say $X_1$ = Location in the city\n",
        "and $X_2$ is number of bedrooms.\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1UzMzPv01R8CcRHUsmn_MvTSTi2iuT9jW)\n",
        "\n",
        "\n",
        "Think of fitting a function to these red dots.\n",
        "\n",
        "The real data could have many different kinds of structures. Just because we think of it as a linear one doesn't mean that a true structure is linear.\n",
        "So the real data may or may not have a linear structure.\n",
        "But let's say out of the many possible functions that we could fit, we choose to fit the class of linear functions because already there are many, many possible linear functions that I could fit for different values of w and b.\n",
        "\n",
        "Linear regression would cut these points in a way that minimizes the distance of the points from the plane.\n",
        "Predicting the target accurately using this linear model would require us to find values (w,b) that minimize the average ditsance to the hyperplane of each sample in the training dataset. we write this as an objective function.\n",
        "\n",
        "$$\n",
        "l(w,b) = \\frac{1}{2n} \\sum_{i=1}{n} (y^i - \\hat{y^i})^2\n",
        "$$\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=10mtSlNzXI1vd_2cO9rlPLR-4d6Ebh-lK)\n",
        "\n",
        "we divide by n to tale the average error per sample. and by 2 purely for mathematical convenience when taking derivatives.\n",
        "it does not change which (w,b) minimizes the loss.\n",
        "\n",
        "### why divide by n?\n",
        "\n",
        "$$\n",
        "l(w,b) = \\frac{1}{n} \\sum_{i=1}{n} (y^i - \\hat{y^i})^2\n",
        "$$\n",
        "\n",
        "this makes the loss: indepedent of the dataset size, Interpretable as \"average squared error per data point and stable when comparing across datasets and keeps gradients from scaling with n.\n",
        "\n",
        "If you double your dataset, you don't want your loss to automatically double just because you have more points.\n",
        "\n",
        "So dividing by n is about normalization.\n",
        "\n",
        "Anoher Clarification with ChatGPT:\n",
        "In a single training dataset, you usually have a bunch of examples (data points). for example, maybe you have 1000 training examples. By diving by n, you're just taking teh total loss over all those 1000 examples and turning it into an average loss per example in that dataset.\n",
        "\n",
        "Why it matters, if you add more data points to your training set, you dont want the loss to just blow up in magnitude. by averaging, you keep the loss on a consistent scale no matter how many data points you have.\n",
        "\n",
        "\n",
        "### why divide by 2?\n",
        "\n",
        "Video Lecture on derivatives: https://www.youtube.com/watch?v=2ziKb3lv4aA\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1BvFifv-Z4jR_NKhoWpQNiIeywABMFtCF)\n",
        "\n",
        "\n",
        "Math term: a derivative measures the instantaneous rate of change of a function. essentially telling you how fast a function's output changes as its input changes\n",
        "\n",
        "If f(x) = $ x^2 $, it's derivative $f'(x) = 2x $. This means the slope of the $x^2 $ curve at x = 3 = 2(3) = 6.\n",
        "\n",
        "Another important concept is Differentiation, which is the mathematical process of finding the derivative, which measures the instantaneous rate of change of a function. for example, if you have a function like:\n",
        "$$\n",
        "f(z) = z^2\n",
        "$$\n",
        "\n",
        "and we take its derivative with respect to z, we get: $ f'(z) = 2z$\n",
        "\n",
        "So when we differentiate $z^2$ a 2 naturally appears.\n",
        "\n",
        "\n",
        "Since we have a squaring in the loss function i.e.\n",
        "$$\n",
        "(y^i - \\hat{y^i})^2\n",
        "$$\n",
        "\n",
        "When we take the derivative of thta we respect to the model parameters (like weights w), the chain rule brings down a factor of 2. So if our loss is something like:\n",
        "$$\n",
        "l(w,b) = \\frac{1}{n} \\sum_{i=1}{n} (y^i - \\hat{y^i})^2\n",
        "$$\n",
        "\n",
        "then when we take the derivative, we get a “2” from that squared term. That extra “2” can make the derivative look a bit messy.\n",
        "\n",
        "✨ Why divide by 2 in the original loss?\n",
        "\n",
        "By dividing the whole loss by 2 upfront, that “2” cancels out when we differentiate. It’s just a neat algebraic trick to make the gradient a little cleaner and easier to work with."
      ],
      "metadata": {
        "id": "5VSIAUlmiAJl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n-RTdqTXh_2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10- Linear Regression: Loss Function & Optimization  - UPENN Deep Leanring by Professor Pratik Chaudhari\n",
        "\n",
        "Linear Regression Models job is to build a model where $\\hat{y^i}$ which is our prediction should be as close to the true target as we can get them to be.\n",
        "\n",
        "We like to think of something called as an objective function. An objective function is a function of the inputs and outputs, the training data set, and the parameters. The parameters are implicit its w and b. y hat is a function of w and b. So, it's a function of three things-- the training data set, your parameters w, and your parameters b.\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1fc99161HYNga_bs45lQWV9UrU8cxIBdk)\n",
        "\n",
        "Different Loss Functions\n",
        "\n",
        "## 1- Squared Error (L2 Loss)\n",
        "\n",
        "$$\n",
        "(\\hat{y}(x^i) - y^i)^2\n",
        "$$\n",
        "\n",
        "## 2- Absolute Error (L1 Loss)\n",
        "\n",
        "$$\n",
        "|\\hat{y}(x^i) - y^i)|\n",
        "$$\n",
        "\n",
        "Both measure the difference between prediction and the truth, but they penalizr mistake differently.\n",
        "\n",
        "Let:\n",
        "\n",
        "$$\n",
        "e = \\hat{y}(x^i) - y^i\n",
        "$$\n",
        "\n",
        "so the losses are:\n",
        "\n",
        "* squared loss: $e^2$\n",
        "* Absolute loss: $|e|$\n",
        "\n",
        "quick note $\\hat{y}(x^i) $ is same as $\\hat{y^i}$\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1118YQjv5hF80S4zyaZVkbZ1cLlz5-7mG)\n",
        "\n",
        "  on the x-axis, we have error plotted and on the y-axis: we have loss.\n",
        "\n",
        "  So the graph shows how loss changes as error changes.\n",
        "\n",
        "## Squared Loss Shape $e^2$\n",
        "That U-shaped curve is: loss = $e^2$\n",
        "\n",
        "Properties:\n",
        "* Minimum at e = 0\n",
        "* symmetric\n",
        "* Grows very fast when error increases\n",
        "* Big mistakes are punished a lot\n",
        "\n",
        "This is why sqaured error loss very sensitive to outliers.\n",
        "\n",
        "## Absolute Loss Shape $|e|$\n",
        "This is a V-shaped.\n",
        "Properties:\n",
        "* Minimum at e = 0\n",
        "* Grows linearly  \n",
        "* Big errors are not exaggerated\n",
        "\n",
        "Both measure prediction error, both are minimized when predictions are perfect, they lead to different optimization behavior.\n",
        "\n",
        "#Pick up the loss fiunction:\n",
        "\n",
        "Any ideas about which one I should do if I want to predict the price of housing in Philly? There are houses with, like, 7 bathrooms sometimes. Is the cost of that house 7 times more than in the cost of your house? No? So the cost of houses plateaus out sometime, right? So we would like functions that kind of are like this. They don't grow arbitrarily large as you keep increasing the number of bathrooms a lot.\n",
        "And this is really where you use your understanding of the problem to pick a loss. You can always pick a quadratic loss. But you know that for this particular case, the quadratic loss may not be a reasonable choice. There could be slightly better choices.\n",
        "\n",
        "\n",
        "So far, many , of what we have written down is I have an objective. This is \"l\" of w,b. So in this course, we'll always use \"l\" for the objective. We will not give a name for the loss function so much because we don't need it as much. \"l\" will be the objective. It is a function of two things, W's and B's. It is also a function of the training data set.\n",
        "\n",
        "Fitting linear regression or fitting any objective involves you finding the best w and the best b that minimizes l. So it is the R minimum of W's that are all D dimensional vectors because our inputs x are D dimensional vectors And b is a real value.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hhQJ-u_0Tn2z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NvV_WX9n52Os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11- Linear Regression - Linear Regression in 3 Minutes\n",
        "\n",
        "Video: Quick Intro of linear Regression: https://www.youtube.com/watch?v=3dhcmeOTZ_Q\n",
        "\n",
        "## W - weight vector\n",
        "\n",
        "\n",
        "$ \\beta_1 $ controls the slope of the line and $ \\beta_0 $\n",
        "\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1bTv8_BDz528BwYtrzY9i55F5PmhKJU50)\n",
        "\n",
        "  and $ \\beta_0 $ controls the intercept of the line: Also known as bias.\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=11yRlGMCC59i_yXVsqKq0mUaLMAKlJjBP)\n",
        "\n",
        "These two coefficients are what we are solving for in linear regression.\n",
        "\n",
        "We can also extend multiple input variables:\n",
        "\n",
        "$$\n",
        "y = \\beta_4 x_4 + \\beta_3 x_3 + \\beta_2 x_2 + \\beta_1 x_1 +\\beta_0\n",
        "$$\n",
        "\n",
        "so on- acting as slopes for each of those variables in these higher dimension.\n",
        "\n",
        "I know - its hard to visualization multiple dimensional linear regression - but I am trying to understand through data - each slope in a multiple dimension - is just how much of change occur if x increases by 1 unit\n",
        "\n",
        "Yes — that statement is exactly right. You’re not missing anything.\n",
        "\n",
        "In multiple linear regression, each slope means:\n",
        "\n",
        "how much the prediction changes when that specific feature increases by 1 unit, while all other features are held constant.\n",
        "\n",
        "Let’s formalize it and then anchor it to intuition.\n",
        "\n",
        "$$\n",
        "y = \\beta_4 x_4 + \\beta_3 x_3 + \\beta_2 x_2 + \\beta_1 x_1 +\\beta_0\n",
        "$$\n",
        "\n",
        "Each $\\beta_i$ is a slope and answers this question: If I increase $x_i$ by 1, and freeze all other $x_j$'s how much does y change?\n",
        "\n",
        "$$\n",
        " \\beta_i = \\frac{\\delta y}{\\delta x_i}\n",
        "$$\n",
        "\n",
        "so yes - it is literally the rate of change of the output with respect to that feature.\n",
        "\n",
        "### Data-level intuition\n",
        "Suppose:\n",
        "\n",
        "$$\n",
        "y = 3x_i + 0.5 x_2 + 10 x_3 + b\n",
        "$$\n",
        "\n",
        "Then:\n",
        "* +1 in $x_1$ -> y increases by 3\n",
        "* +1 in $x_2$ -> y descreases by 0.5\n",
        "* +1 in $x_3$ -> y increases by 10\n",
        "\n",
        "Each dimension has its own slope.\n",
        "\n",
        "Instead of a line -we now have a hyperplane, Each weight controls the tilt of that hyperplane along one axis.\n",
        "\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1CF9mfTdGaEFlCis08n0qEacnlGt00A0l)\n",
        "\n",
        "# Fitting a line:\n",
        "\n",
        "so how do we fit the line through a regression model.\n",
        "there can be many ways we can fit a line from across these red dots and but we select the one that minimizes the square of residuals of all the dots from the line.\n",
        "\n",
        "\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1OIlz9LqiYvvXAOPjjna_33axE5TJ7ueM)\n",
        "\n",
        "the squaring ampliefies the differences.\n",
        "If we total the areas of all of these squares for a given line -  we will get the sum of squared error and this is known as our lost function.\n",
        "\n",
        "Again, we find the $\\beta_0$ and $ \\beta_1 $ co-efficients that will minimize that sum of squared error:\n",
        "\n",
        "The co-efficients can be solved with a variety of techniques for example\n",
        "matrix decomposition to gradient descent.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Gbo4Z8-P4G8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12- Perceptron\n",
        "\n",
        "Perceptron is a simple model, it's a classification model.\n",
        "Frank Rosenblatt had built a machine to distinguish punch cards that are punched left and punch cards that are punched on the right-hand side. The very first Classification Model.\n",
        "\n",
        "A perceptron is a classification machine. Unlike Regression Model, which was a regressor, a perceptron predicts +1 or - , given an input x\n",
        "\n",
        "so lets say it takes input for exampel an image x as a d-dimensional vector and it takes w - which are d-dimensional parameters. It applies w transpose x.\n",
        "\n",
        "$$\n",
        " f(x;w) = sign(w^T x)\n",
        "$$\n",
        "\n",
        "So upto this point - its exactly like a linear regression, except taht isntead of predicting the value directly, it applies a sign function.\n",
        "\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1TPpnYmdJRlX7O7IJ7Ho-GIbrKRAM6Z4c)\n",
        "\n",
        "the above equation is our model for classifier. This is a perceptron.\n",
        "\n",
        "Now, we would like to understand, how should we find the right \"w\" for a perceptron? Just like we focused on how to find the weights of linear regression, we would now like to find the weights of a perceptron. We will fit the perceptron to the training data set. This is simply how the perceptron makes predictions.\n",
        "\n",
        "In this case, we have a much nicer way of saying when a prediction is correct or incorrect. If you predict an apple for an image that is an orange, then obviously, you are wrong. There is no such thing as by how much you are wrong.\n",
        "\n",
        "If you predict the cost of the housing using a linear Regression, there is some notion of how off you are from the true cost. For apples and oranges - classification problem, it is clear. Either you get it correct, or you get it incorrect, for a binary classifier. Right?\n",
        "\n",
        "\n",
        "So, the objective ofa classifier is - we can think of it as 0 or 1 loss. We get penalty of 1 , if we make an incorrect prediction and no penalty or penalty of 0 for correct prediction.\n",
        "\n",
        "below is the Zero-one loss function.\n",
        "\n",
        "## Zero - One Loss\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ofju-6AFop0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13- PCA Principle Component Analysis\n",
        "\n",
        "Stackover notes and explanation of PCA :\n",
        "https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues#comment103261_2691\n",
        "\n",
        "## UPENN Course 5460 Professor Notes on Regression and PCA:\n",
        "\n",
        "PCA is the one that subdivides your data in such a way that the projection of these points on the first eigenvector is minimized. It is slightly different from regression. Regression only cares about the one particular axis, the output axis.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "4Z3xyeZjYBUm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_S9_-PZ55b6U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvRHoeGhjW8ErZBqiVBxDJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFRvMKwmjWtU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "\n",
        "\n",
        "*   PATTERN RECOGNITION\n",
        "AND MACHINE LEARNING\n",
        "CHRISTOPHER M. BISHOОР\n",
        "https://www.microsoft.com/en-us/research/wp-content/uploads/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf\n",
        "\n",
        "*   A LOGICAL CALCULUS OF THE IDEAS IMMANENT IN\n",
        "NERVOUS ACTIVITY* https://www.cs.cmu.edu/~epxing/Class/10715/reading/McCulloch.and.Pitts.pdf\n",
        "\n",
        "\n",
        "* Goodfellow chapter 1\n",
        "https://www.deeplearningbook.org/\n",
        "\n",
        "* Computing machinery and intelligence by Alan Turning in 1960 (Turing, 2009): https://courses.cs.umbc.edu/471/papers/turing.pdf\n",
        "\n",
        "* A LOGICAL CALCULUS OF THE IDEAS IMMANENT IN\n",
        "NERVOUS ACTIVITY  https://www.cs.cmu.edu/~epxing/Class/10715/reading/McCulloch.and.Pitts.pdf\n",
        "\n",
        "\n",
        "2nd Phase of Perceptron -\n",
        "* Learning Internal Representations by Error Propagation Rumelhart et al.\n",
        "https://apps.dtic.mil/sti/html/tr/ADA164453/\n",
        "https://apps.dtic.mil/sti/tr/pdf/ADA164453.pdf\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "sKzE1WihrhpF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Warren McCulloch (Neuroscientist) and Walter Pitts (mathemethician) built the first model of a mechanical neuron and propounded the idea that simple elemental computational blocks in your brain work together to perform complex functions.\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1ymT00yL0lWX3a3fIhKUmP5fF_D3PDhN8)\n",
        "  \n",
        "  Here is the link of their paper: A LOGICAL CALCULUS OF THE IDEAS IMMANENT IN NERVOUS ACTIVITY https://www.cs.cmu.edu/~epxing/Class/10715/reading/McCulloch.and.Pitts.pdf\n",
        "\n",
        "\n",
        "  What McCulloch and Walter interested in was taking these biological neurons and then abstracting it out into some mathematical object that they could understand a little better.\n",
        "\n",
        "  we do not want to necessarily always model such complex flow in order to build a model.\n",
        "\n",
        "  So, what McCulloch and Putts did is thta they said that , let neuron be a simple function. It takes n inputs and produces one output.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7m-N7Ipf3gQz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7n-qAQINm21j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1-Representaton Learning\n",
        "Perceptual agents, from plants to humans, perform measurements of physical processes (\"signals\") ayt a level of granularity that is essentially continuous."
      ],
      "metadata": {
        "id": "TQTTM0ezm3R8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vvj3NtLhnRT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2- Linear Classifier\n",
        "\n",
        "\n",
        "$$\n",
        "f(x;w) = sign(w^T x) = sign(w_1x_1 + .... + w_dx_d)\n",
        "$$\n",
        "\n",
        "This is the equation of a linear classifier and also the mathematical form of a single artificial neuron / perceptron.\n",
        "\n",
        "\n",
        "Where\n",
        "\n",
        "1-  **x** is the data point. Example: a person described by features like height, weight, age, etc.\n",
        "so each $x_i$ is one feature.\n",
        "$$\n",
        "x = (x_1, x_2,x_3,....,x_d)  \n",
        "$$\n",
        "\n",
        "\n",
        "2- w - the weights (parameters to learn)\n",
        "$$\n",
        "w= (w_1, w_2,w_3,....,w_d)  \n",
        "$$\n",
        "These are learned by the model. They represent how important each feature is.\n",
        "\n",
        "3- Big $|w_i|$ feature matters more.\n",
        "\n",
        "\n",
        "$ w^T x $ - weighted sum( dot product)\n",
        "$$ sign(w^T x) = sign(w_1x_1 + .... + w_dx_d) $$\n",
        "\n",
        "This is exactly what neuron's activation before nonlinearity.\n",
        "\n",
        "4- sing - the decision function\n",
        "\n",
        "$$ sign(z) = \\int_{-1\\, if \\,z>0}^{+1 \\,if \\,z>0} $$\n",
        "\n",
        "So after computing the weighted sum, we just check:\n",
        "\n",
        "\n",
        "*   Is it positive? -> class +1\n",
        "*   Is it negative? -> class -1\n",
        "\n",
        "This is exactly how:\n",
        "\n",
        "* Perceptron\n",
        "* Linear classifiers\n",
        "* SVM (core decision rule)\n",
        "* Logistic regression (before sigmoid)\n",
        "\n",
        "start.\n",
        "\n"
      ],
      "metadata": {
        "id": "UuifyqUypRNx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3- Linear Classifier On an Image:\n",
        "The above Linear classifier is also used on image classificagtion as well. When you use this model on an image, your input vector is:\n",
        "$$\n",
        "x = (x_1, x_2,x_3,....,x_d)  \n",
        "$$\n",
        "\n",
        "Each feature $ x_i$ is typically:\n",
        "The pixel value at a specific position in the image.\n",
        "So yes:\n",
        "\n",
        "\n",
        "*   a pixel = a feature\n",
        "*   its position is implicit in the index i\n",
        "*   the model does not see \"row 3, column 5\" - it just sees features #837\"\n",
        "\n",
        "Example: A 64 x 64 grascale image will have 4996 featyres. each $ x_i $ is tge intensity of one pixel. If it's RGB, then each controbutes 3 features (R,G,B)\n",
        "\n",
        "What this model is actually doing to an image is that it computes:\n",
        "$$ w^T x = \\sum_{i} w_i x_i $$\n",
        "Each pixel is multiplied by a learned weight. So the model is effectively learning a template image w and asking \"How similar is this image to my apple-vs.orange template?\"\n",
        "\n",
        "- Pixels that. matter more (edges, color zones, textures) get bigger weights.\n",
        "- Background pixels ideally get weights near zero.\n",
        "\n",
        "Although each raw feature is just a color intensity, the pattern across many pixels encodes:\n",
        "\n",
        "* edges\n",
        "* roundness\n",
        "* texture\n",
        "* shadows\n",
        "* highlights\n",
        "\n",
        "These are emergent features, not explicit ones.\n",
        "\n"
      ],
      "metadata": {
        "id": "dR5-UHnVs1aO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "L07P5Hkm5riW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4- Back-propagation\n"
      ],
      "metadata": {
        "id": "9EjH-NDO52KC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5- CNN - Convolutional Neural Network"
      ],
      "metadata": {
        "id": "3A5lVBZg6Wh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6- Problem Setup for Machine Learning:\n",
        "Nature gives us data X and targets Y for this data\n",
        "$$ X -> Y $$\n",
        "Nature does not usually tell us what property of a datu,m $ X \\in X $ results in a particular orediction $ y \\in Y $. Machine learning is domain where we try to imitate Natures pattern to predict y for a given x by learning through the available data.\n",
        "\n",
        "Machine learning simply implies a notion of being able to identify patterns in the input data without explicitly programming a computer for prediction.\n",
        "\n",
        "we are often happy with a learning process that identifies correlations: If we learn correlations on a few samples\n",
        "$$ (x^1, y^1), ..., (x^n, y^n) $$, we may be able to predict output for a new data $ (x^{n+1}) $\n",
        "\n",
        "We may not need to know why the label of $x^{n+1}$ was predicted to be so and so.\n",
        "\n",
        "Nature possesses a probability distribution P over (X,Y).\n",
        "\n",
        "We will formalize the problem of machine learning as Nature drawing n independent and identically distributed samples from this distribution. This is denoted by:\n",
        "$$\n",
        "D_{train} = \\{(x^i , y^i) \\sim P\\}^{n}_{i=1}\n",
        "$$\n",
        "is called the training set. We use this data to identify patterns that help make predictions on some future data.\n"
      ],
      "metadata": {
        "id": "RdipnCjDxoq2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Designing a predictor that is accurate on $ D_{train} $ is trivial. A hash function that memorizes the data is sufficient. This is NOT our task in machine learning. We want predictors that generalize to new data outside $ D_{train} $.\n",
        "You should never get happy that you get zero error on training data set.\n",
        "\n",
        "\n",
        "The problem in machine learning: Give answer to questions you havent seen before:\n"
      ],
      "metadata": {
        "id": "dIEcNHFvF5me"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7- Generalization:\n",
        "\n",
        "Generalization: A model can predict well on data from the same distribution\n",
        "\n",
        "**Professor**: I shouldn't expect you to be good at answering questions from outside the training data unless there is some shared property in the training and the test data.\n",
        "\n",
        "In machine learning, we talk about one particular kind of shared property, the fact that training and testing may just come from the same probability distribution.\n",
        "\n",
        "The right class of functions f is such that is not too large. Otherwise we will find our binary classifier discussed above as the solution of the problem (after all, it does achieve zero training error). This is not very useful. The class of functions that we should search over cannot be too small either, otherwise we won’t be able to make accurate predictions for difficult images."
      ],
      "metadata": {
        "id": "Y-ksGtetICQi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8- Linear Regression: Reference Lectures\n",
        "\n",
        "* Quick Intro of linear Regression: https://www.youtube.com/watch?v=3dhcmeOTZ_Q\n",
        "* GPA Example - Linear Regression: https://www.youtube.com/watch?v=Muw2GjZ0Xww\n",
        "\n",
        "#9 - Linear Regresssion - UPenn ES5460\n",
        "\n",
        "We usually have images or input data in d dimension vector and our labels are simply real values in regression problem.\n",
        "\n",
        "Regression problems are simply problems that have some inputs, and we expect to regress the output, to find a real value number that is close to the true real value number that was given to you as a part of the training data set.\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1Bs7Bmuz7V7JSCC9fOqxr-VO6qMuW0teE)\n",
        "\n",
        "\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1xf6TDCNHTdEdANiFWthrvNw_NXAmHBSb)\n",
        "\n",
        "\n",
        "So, Machine Learning function is:\n",
        "\n",
        "$$\n",
        "f(x; w,b) = w^T + b\n",
        "$$\n",
        "\n",
        "B is an scalar value ( in lay man : That’s it. One value. Not a list. Not a vector. Not a matrix.)\n",
        "the function take x input and is parameterize by w and b and outputs y.\n",
        "\n",
        "## Example of Regression model:\n",
        "In Below diagram we have two dimensions - Lets say $X_1$ = Location in the city\n",
        "and $X_2$ is number of bedrooms.\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1UzMzPv01R8CcRHUsmn_MvTSTi2iuT9jW)\n",
        "\n",
        "\n",
        "Think of fitting a function to these red dots.\n",
        "\n",
        "The real data could have many different kinds of structures. Just because we think of it as a linear one doesn't mean that a true structure is linear.\n",
        "So the real data may or may not have a linear structure.\n",
        "But let's say out of the many possible functions that we could fit, we choose to fit the class of linear functions because already there are many, many possible linear functions that I could fit for different values of w and b.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5VSIAUlmiAJl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n-RTdqTXh_2W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}